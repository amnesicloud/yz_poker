# 德扑项目设计

## 背景
作为一个德扑上瘾技术又差 + 围棋上瘾技术又差的算法工程师，在目睹alphaGo之后，心里一直在想是否可以写一个类似的算法。CMU的pluribus已经证明了该想法的可行性，但是目前网上只找到了其相关的论文，没有办法用于指导教学。

因此，我希望设计开发一个可解释性较强的机器学习模型，用于指导自己日常的菜鸡互啄（被啄）。

## 设计理念
1. 可解释性

    我定义可解释性为，可以被我这个菜鸡理解的“action”的“原因”。
    
    “action”为德扑中的call、fold、bet，其中bet可能又需要根据bet size、2-bet 3-bet等继续分类；
    
    “原因”则是condition在某一环境、某一目的下的一个普遍适用的一个最佳思考方式。
    
    解释一下这句比较nerd的话：我认为思考方式是一个先验在action之上的概率分布，最佳的思考方式则是在（目的、环境）的后验上最大化目的的预期收益的选择（MAP的思路）。
    
    再举例解释一下这个更nerd的解释…：

    我认为“如果一桌人都打的比较松，那么我希望该模型可以学会变的紧一些；如果一桌人都打的比较紧，那么我会希望该模型可以变的松一些”是一种思考方式。

    在这种描述下，同桌人的行为就是环境，那么“目的”可以是“赢得最多的钱”或“成为最终的赢家”（现金局与sng局的不同），以现金局的“赢得最多的钱”为目的时，那么一个可能的“思考方式”即是上述的思考方式。紧和松可能可以用德扑中的range + bet size/action来衡量，我希望这个模型能够告诉我，在面对什么样的人的时候，我应该采取什么样的action，能够最好地完成我“赢得最多的钱”的目的。

2. 价值衡量：长期价值、重尾分布与终局时间

    作为一个菜鸡，我听到最多的一句话就是：长期来看，这种打法会赢钱/输钱，即人们认为，如果重复很多次，其收益会收敛，也即是所谓的长期价值、期望价值。

    作为一个在统计学上刚刚入门的菜鸡，我觉得上面这句话使用了非常理想主义的中心极限定律，即认为存在一种理想的环境，双方有无限的筹码与时间，可以一直打牌打下去。只有在这种情况下，任意的action的期望收益比例会收敛至某一确定的均值上。
    
    作为一个建模方式，这种简单的抽象无疑是有效的第一步，但是如果希望建模这个复杂的世界，我觉得需要迈出第二步。
    
    我认为需要迈出的第二步是确定权重。即，“action”的“收益权重”是不一样的，收益权重由胜率 + 价格权重确定而来，价格权重是由pot size和筹码量比例决定的。
    
    举例解释：考虑我有50个bb的情况，当对手在有40个bb和4000个bb的不同情况时，其action是一定有差异的，本质原因是action的收益权重是不一样的。
    
    再举例说明：为什么sng局中涨盲后会经常地出现all-in的情况？我认为这是明显地代表了，由于盲注地上升，选手对于比赛终止时间的预期大幅缩短。这种情况下，使用第一步中“无限的时间”的假设是明显不可取的。

    通过加入收益权重的办法，我认为可以增大小概率事件发生的影响，从而影响action的价值判断。本质上是将小概率事件从可近似忽略的细尾分布变为重尾分布。
    
    并且德扑最终只有一个赢家，而小概率事件会导致更快地收敛至一个赢家的终态，那我希望这个模型可以更好地学习、利用这种小概率事件。即，当模型在听nuts牌，outs很少的情况下，这个时候虽然胜率低，但是听到牌之后能够获得的价值也高。虽然现在的牌手对于这种价值有一定的近似，但是精确的计算还是应当交给模型。
    
    所以我认为的建模的第二步在于，小概率事件 + 终态（最终只有一个人获胜）对于action的影响。

    我希望我的模型可以告诉我，考虑了这两点之后，什么样的价值衡量（如胜率、EV等类似衡量手段），是最应该被考虑的。

## 框架设计

整体来说，这应该是一个强化学习模型，整体应当包括基础架构 + 强化学习模型 + on-the-fly的输入/inference共3个模块。

1. 首先是基础建设，包括所有确定性的功能

    i. 德扑的环境。其中包含随机发牌，终局判断，partial的信息（即对手手牌如果没有亮出来，不能作为智能体学习的输入）

    ii. 基础预估模块。预估模块将包含range，手牌对比的胜率预估。

    iii. action的合法性（check、fold、bet）

2. 其次是模型部分，即模型的输入与输出。

    模型的输入包括：

    i. 全局环境信息/全局历史信息 (C_past_env)：

        筹码信息(筹码比例，盲注大小)，对手历史信息（预估的入池、bet range，）

    ii. 当前局的历史信息(C_curr_env)

        结合即本轮牌局之前的其他选手的action序列，需要可以产出当前对手的value range, bluff range, semi-bluff range. 即，flop时预估pre-flop的情况，turn时预估pre-turn的情况，river时预估pre-river的情况。

    iii. 本轮的信息(C_curr_hands)

        结合本轮对手的行为，产出本轮对手的range，value-range，bluff range等。

    输出则是action，即check，fold，bet（bet需要给出bet size）

3. on-the-fly 的 inference模块

    主要目的是，希望对于任意一个牌局来说，range的权重可以更新(一共只有1326种情况)，但是更新方式将是模型确定好的。

## 模型、初始化与训练方式

1. 模型

	模型主要由以下的小模型构成：

        i. 对于对手思路的模拟

        每个对手具有一个可训练的“打法”隐向量。打法隐向量 + 筹码情况 + 位置决定了他的“思路”；“思路”可分为“value”、“bluff”、“semi-bluff”的三个决策隐向量（成牌、诈和、听牌） + 向量权重；“思路”向量 + 向量权重将决定action。

        ii. 自身的决策方法

        待更新

2. 初始化

    待更新

3. 我个人认为需要使用curriculum learning的方式帮助收敛，即首先训练河牌圈，然后是转牌，直到pre-flop。从结果出发有助于模型意识到牌的大小。



    